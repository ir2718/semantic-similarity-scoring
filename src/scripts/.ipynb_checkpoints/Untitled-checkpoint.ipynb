{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aae70add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/Ivan/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460af57f3e644b3bb32b7dbaacc6804f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15880\\510654660.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFROZEN_CFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m tokenizer_kwargs = {\n\u001b[0;32m     36\u001b[0m     \u001b[1;34m'tokenizer'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         \u001b[0mtokenizer_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m         \u001b[0mconfig_tokenizer_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tokenizer_class\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[0mtokenizer_auto_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[0mtokenizer_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tokenizer-test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m     ```\"\"\"\n\u001b[1;32m--> 359\u001b[1;33m     resolved_config_file = get_file_from_repo(\n\u001b[0m\u001b[0;32m    360\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mget_file_from_repo\u001b[1;34m(path_or_repo, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[1;31m# Load from URL or cache if already cached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         resolved_file = cached_path(\n\u001b[0m\u001b[0;32m    679\u001b[0m             \u001b[0mresolved_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m         output_path = get_from_cache(\n\u001b[0m\u001b[0;32m    283\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    543\u001b[0m                     )\n\u001b[0;32m    544\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m    546\u001b[0m                         \u001b[1;34m\"Connection error, and we cannot find the requested files in the cached path.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m                         \u001b[1;34m\" Please try again or make sure your Internet connection is on.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from ray import tune\n",
    "from utils import *\n",
    "from parsing import parse_fine_tune\n",
    "from configs import FROZEN_CFG\n",
    "import os\n",
    "\n",
    "class Args:\n",
    "    model_name='bert-base',\n",
    "    num_epochs=10\n",
    "    train_batch_size=32\n",
    "    val_batch_size=64\n",
    "    weight_decay=0.001\n",
    "    learning_rate_start=1e-4\n",
    "    optim='adamw_torch'\n",
    "    max_len=512\n",
    "    seed=42\n",
    "    patience=3\n",
    "    warmup_ratio=0.1\n",
    "    scheduler='cosine'\n",
    "    pretrained_path=None\n",
    "\n",
    "args = Args()\n",
    "FROZEN_CFG.set_args(args)\n",
    "\n",
    "set_seed_(FROZEN_CFG.SEED)\n",
    "\n",
    "device = set_device()\n",
    "\n",
    "dataset = load_dataset_from_huggingface(DATASET_PATH, CONFIG_NAME)\n",
    "model_name = FROZEN_CFG.MODEL_NAME\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_kwargs = {\n",
    "    'tokenizer': tokenizer,\n",
    "    'max_len': FROZEN_CFG.MAX_LEN\n",
    "}\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    fn_kwargs=tokenizer_kwargs,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def model_init():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        num_labels=1,\n",
    "    )\n",
    "    freeze_encoder(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=len(dataset['train']),\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    optim=FROZEN_CFG.OPTIM,\n",
    "    learning_rate=FROZEN_CFG.LEARNING_RATE_START,\n",
    "    per_device_train_batch_size=FROZEN_CFG.TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=FROZEN_CFG.VAL_BATCH_SIZE,\n",
    "    num_train_epochs=FROZEN_CFG.EPOCHS,\n",
    "    output_dir=os.path.join('../frozen', model_name),\n",
    "    weight_decay=FROZEN_CFG.WEIGHT_DECAY,\n",
    "    lr_scheduler_type=FROZEN_CFG.SCHEDULER,\n",
    "    warmup_ratio=FROZEN_CFG.WARMUP_RATIO,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=None,\n",
    "    args=training_args,\n",
    "    model_init=model_init,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=FROZEN_CFG.PATIENCE)]\n",
    ")\n",
    "\n",
    "hp_space = {\n",
    "    'learning_rate': tune.choice([1e-3, 5e-4, 1e-4, 5e-5, 1e-5]),\n",
    "    'per_device_train_batch_size': tune.choice([8, 16, 32]),\n",
    "    'weight_decay': tune.choice([1e-2, 1e-3, 1e-4])\n",
    "}\n",
    "\n",
    "scheduler = PopulationBasedTraining(\n",
    "    time_attr='training_iteration',\n",
    "    mode='max',\n",
    "    metric='objective',\n",
    ")\n",
    "\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    hp_space=lambda _: hp_space,\n",
    "    direction='maximize',\n",
    "    backend='ray',\n",
    "    compute_objective=compute_objective,\n",
    "    scheduler=scheduler,\n",
    "    keep_checkpoints_num=1,\n",
    "    verbose=0,\n",
    "    reuse_actors=True,\n",
    "    n_trials=10,\n",
    ")\n",
    "\n",
    "print('BEST TRIAL: ')\n",
    "print(best_run)\n",
    "\n",
    "print('starting finetuning with frozen encoder . . .')\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer.args, n, v)\n",
    "\n",
    "train_output = trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
