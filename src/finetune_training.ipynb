{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd30f946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ray[tune] in c:\\users\\ivan\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.43.0)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (3.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.20.3)\n",
      "Requirement already satisfied: virtualenv in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (20.16.5)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.15.3 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (3.20.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (6.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.0.2)\n",
      "Requirement already satisfied: attrs in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (21.2.0)\n",
      "Requirement already satisfied: frozenlist in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.3.1)\n",
      "Requirement already satisfied: click<=8.0.4,>=7.0 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (8.0.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ivan\\appdata\\roaming\\python\\python39\\site-packages (from ray[tune]) (3.7.0)\n",
      "Requirement already satisfied: aiosignal in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.2.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.3.4)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (2.5.1)\n",
      "Requirement already satisfied: tabulate in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ivan\\appdata\\roaming\\python\\python39\\site-packages (from click<=8.0.4,>=7.0->ray[tune]) (0.4.4)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from grpcio<=1.43.0,>=1.28.1->ray[tune]) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from jsonschema->ray[tune]) (0.18.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from jsonschema->ray[tune]) (58.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from pandas->ray[tune]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from pandas->ray[tune]) (2021.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (2.0.4)\n",
      "Requirement already satisfied: distlib<1,>=0.3.5 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from virtualenv->ray[tune]) (0.3.6)\n",
      "Requirement already satisfied: platformdirs<3,>=2.4 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from virtualenv->ray[tune]) (2.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install ray[tune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9028b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ivan\\AppData\\Local\\Temp/ipykernel_12552/1382550860.py:6: DeprecationWarning: The module `ray.tune.suggest` has been moved to `ray.tune.search` and the old location will be deprecated soon. Please adjust your imports to point to the new location. Example: Do a global search and replace `ray.tune.suggest` with `ray.tune.search`.\n",
      "  from ray.tune.suggest.hyperopt import HyperOptSearch\n",
      "C:\\Users\\Ivan\\AppData\\Local\\Temp/ipykernel_12552/1382550860.py:6: DeprecationWarning: The module `ray.tune.suggest.hyperopt` has been moved to `ray.tune.search.hyperopt` and the old location will be deprecated soon. Please adjust your imports to point to the new location. Example: Do a global search and replace `ray.tune.suggest.hyperopt` with `ray.tune.search.hyperopt`.\n",
      "  from ray.tune.suggest.hyperopt import HyperOptSearch\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoConfig, DataCollatorWithPadding, TrainingArguments, Trainer, TrainerCallback\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score, mean_squared_error\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7baae1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    MODEL_NAME = 'roberta-large'\n",
    "    PRETRAINED_PATH = ''\n",
    "    \n",
    "    EPOCHS = 10\n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    VAL_BATCH_SIZE = 64\n",
    "    WEIGHT_DECAY = 0.001\n",
    "    LEARNING_RATE_START = 1e-4\n",
    "    WARMUP_RATIO = 0.1\n",
    "    SCHEDULER_TYPE = 'cosine'\n",
    "    \n",
    "    SEED = 42\n",
    "    \n",
    "def set_seed_(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed_(CFG.SEED)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d382872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Ivan\\.cache\\huggingface\\datasets\\glue\\stsb\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c06fd422034661a1a066686c9b7f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x000002268BCB34C0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5589dd8d7cb44ff9ac62b892bacbafda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1d4f1ec6e94c35ad8f78883da98575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3276d8c21ce429ebc18e2b0c9da0cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('glue', 'stsb')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CFG.MODEL_NAME, \n",
    "    #max_length=CFG.MAX_LEN\n",
    ")\n",
    "\n",
    "def tokenize_function(ex):\n",
    "    return tokenizer(\n",
    "        ex['sentence1'], \n",
    "        ex['sentence2'], \n",
    "        truncation=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd2af3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(CFG.MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_config(config)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CFG.MODEL_NAME,\n",
    "    num_labels=1,\n",
    ")\n",
    "\n",
    "def unfreeze_encoder(model):\n",
    "    for p in model.base_model.encoder.parameters():\n",
    "        p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae811864",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mixed precision training with AMP or APEX (`--fp16` or `--bf16`) and half precision evaluation (`--fp16_full_eval` or `--bf16_full_eval`) can only be used on CUDA devices.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12552/3846085997.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     }\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m training_args_fine_tune = TrainingArguments(\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mevaluation_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0msave_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_on_each_node, no_cuda, seed, data_seed, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, xpu_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, deepspeed, label_smoothing_factor, optim, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    935\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp16\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp16_full_eval\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbf16\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbf16_full_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m         ):\n\u001b[1;32m--> 937\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    938\u001b[0m                 \u001b[1;34m\"Mixed precision training with AMP or APEX (`--fp16` or `--bf16`) and half precision evaluation (`--fp16_full_eval` or `--bf16_full_eval`) can only be used on CUDA devices.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: Mixed precision training with AMP or APEX (`--fp16` or `--bf16`) and half precision evaluation (`--fp16_full_eval` or `--bf16_full_eval`) can only be used on CUDA devices."
     ]
    }
   ],
   "source": [
    "## TRAINING END TO END ##\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    output, labels = eval_preds\n",
    "    output = output.reshape(-1)\n",
    "    return {\n",
    "        'pearson_r': pearsonr(output, labels)[0],\n",
    "        'spearman_r': spearmanr(output, labels)[0]\n",
    "    }\n",
    "\n",
    "training_args_fine_tune = TrainingArguments(\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    learning_rate=CFG.LEARNING_RATE_START,\n",
    "    per_device_train_batch_size=CFG.TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=CFG.VAL_BATCH_SIZE,\n",
    "    num_train_epochs=CFG.EPOCHS,\n",
    "    output_dir=os.path.join('./finetune', CFG.MODEL_NAME),\n",
    "    weight_decay=CFG.WEIGHT_DECAY,\n",
    "    lr_scheduler_type=CFG.SCHEDULER_TYPE,\n",
    "    warmup_ratio=CFG.WARMUP_RATIO,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "unfreeze_encoder(model)\n",
    "trainer_fine_tune = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction='maximize',\n",
    "    backend='ray',\n",
    "    search_alg=HyperOptSearch(metric='objective', mode='max'),\n",
    "    scheduler=PopulationBasedTraining(metric='objective', mode='max')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a65b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add training with best hyperparams\n",
    "\n",
    "#trainer_fine_tune.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03a8439",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(tokenized_datasets['test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
