{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a40070f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ray[tune] in c:\\users\\ivan\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ivan\\appdata\\roaming\\python\\python39\\site-packages (from ray[tune]) (3.7.0)\n",
      "Requirement already satisfied: attrs in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (21.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (2.26.0)\n",
      "Requirement already satisfied: click<=8.0.4,>=7.0 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (8.0.3)\n",
      "Requirement already satisfied: frozenlist in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.3.1)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.15.3 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (3.20.1)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (3.2.0)\n",
      "Requirement already satisfied: aiosignal in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.2.0)\n",
      "Requirement already satisfied: virtualenv in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (20.16.5)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.20.3)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.0.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (6.0)\n",
      "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.43.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (0.9.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.3.4)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (2.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ivan\\appdata\\roaming\\python\\python39\\site-packages (from click<=8.0.4,>=7.0->ray[tune]) (0.4.4)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from grpcio<=1.43.0,>=1.28.1->ray[tune]) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from jsonschema->ray[tune]) (0.18.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ivan\\appdata\\roaming\\python\\python39\\site-packages (from jsonschema->ray[tune]) (62.3.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from pandas->ray[tune]) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from pandas->ray[tune]) (2.8.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (2021.10.8)\n",
      "Requirement already satisfied: distlib<1,>=0.3.5 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from virtualenv->ray[tune]) (0.3.6)\n",
      "Requirement already satisfied: platformdirs<3,>=2.4 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from virtualenv->ray[tune]) (2.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install ray[tune]\n",
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed0a93c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM, AutoConfig, DataCollatorForLanguageModeling, TrainingArguments, Trainer, TrainerCallback\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, accuracy_score, mean_squared_error\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from datasets import load_dataset\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4587bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    MODEL_NAME = 'roberta-large'\n",
    "    \n",
    "    EPOCHS = 15\n",
    "    TRAIN_BATCH_SIZE = 1\n",
    "    VAL_BATCH_SIZE = 2\n",
    "    WEIGHT_DECAY = 0.001\n",
    "    LEARNING_RATE_START = 1e-4\n",
    "    \n",
    "    #MAX_LEN = 512\n",
    "    BLOCK_SIZE = 10\n",
    "    \n",
    "    SEED = 42\n",
    "    \n",
    "def set_seed_(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed_(CFG.SEED)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "287236f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Ivan\\.cache\\huggingface\\datasets\\glue\\stsb\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('glue', 'stsb')\n",
    "\n",
    "def get_all_sentences_list(dataset, split):\n",
    "    return dataset[split]['sentence1'][:] + dataset[split]['sentence2'][:]\n",
    "\n",
    "all_sentences_train = get_all_sentences_list(dataset, 'train')\n",
    "all_sentences_validation = get_all_sentences_list(dataset, 'validation')\n",
    "all_sentences_test = get_all_sentences_list(dataset, 'test')\n",
    "\n",
    "dataset_mlm = datasets.DatasetDict({\n",
    "    'train': datasets.Dataset.from_dict({'sentence': all_sentences_train}),\n",
    "    'validation': datasets.Dataset.from_dict({'sentence': all_sentences_validation}),\n",
    "    'test': datasets.Dataset.from_dict({'sentence': all_sentences_test})\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d0abacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x0000023434973040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 11498\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 2758\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CFG.MODEL_NAME, \n",
    "    #max_length=CFG.MAX_LEN\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples['sentence'])\n",
    "    if tokenizer.is_fast:\n",
    "        result['word_ids'] = [result.word_ids(i) for i in range(len(result['input_ids']))]\n",
    "    return result\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset_mlm.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=['sentence']\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f488ae6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop\n",
    "    total_length = (total_length // CFG.BLOCK_SIZE) * CFG.BLOCK_SIZE\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + CFG.BLOCK_SIZE] for i in range(0, total_length, CFG.BLOCK_SIZE)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_labeled_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30b45528",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    CFG.MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42ac248f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n",
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "C:\\Users\\Ivan\\anaconda3\\lib\\site-packages\\accelerate\\memory_utils.py:23: FutureWarning: memory_utils has been reorganized to utils.memory. Import `find_executable_batchsize` from the main `__init__`: `from accelerate import find_executable_batch_size` to avoid this warning.\n",
      "  warnings.warn(\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Ivan\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 11498\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 21570\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='21570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    3/21570 00:00 < 1:16:34, 4.69 it/s, Epoch 0.00/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 11498\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 43125\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 11498\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 86235\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 11498\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 172470\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No executable batch size found, reached zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33748/3962850494.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1315\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m         )\n\u001b[1;32m-> 1317\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1318\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\accelerate\\utils\\memory.py\u001b[0m in \u001b[0;36mdecorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No executable batch size found, reached zero.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No executable batch size found, reached zero."
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    learning_rate=CFG.LEARNING_RATE_START,\n",
    "    num_train_epochs=CFG.EPOCHS,\n",
    "    weight_decay=CFG.WEIGHT_DECAY,\n",
    "    output_dir=os.path.join('./masked_lm', CFG.MODEL_NAME),\n",
    "    fp16=True,\n",
    "    #auto_find_batch_size=True,\n",
    "    #gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c75b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = trainer.predict(tokenized_datasets['test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
