{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb557a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ray[tune] in c:\\users\\ivan\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: frozenlist in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.3.1)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.20.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ivan\\appdata\\roaming\\python\\python39\\site-packages (from ray[tune]) (3.7.0)\n",
      "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.43.0)\n",
      "Requirement already satisfied: aiosignal in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.2.0)\n",
      "Requirement already satisfied: attrs in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (21.2.0)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.15.3 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (3.20.1)\n",
      "Requirement already satisfied: virtualenv in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (20.16.5)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (2.26.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (6.0)\n",
      "Requirement already satisfied: click<=8.0.4,>=7.0 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (8.0.3)\n",
      "Collecting tabulate\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.3.4)\n",
      "Collecting tensorboardX>=1.9\n",
      "  Using cached tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ivan\\appdata\\roaming\\python\\python39\\site-packages (from click<=8.0.4,>=7.0->ray[tune]) (0.4.4)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from grpcio<=1.43.0,>=1.28.1->ray[tune]) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from jsonschema->ray[tune]) (0.18.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ivan\\appdata\\roaming\\python\\python39\\site-packages (from jsonschema->ray[tune]) (62.3.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from pandas->ray[tune]) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from pandas->ray[tune]) (2.8.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (1.26.7)\n",
      "Requirement already satisfied: platformdirs<3,>=2.4 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from virtualenv->ray[tune]) (2.5.2)\n",
      "Requirement already satisfied: distlib<1,>=0.3.5 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from virtualenv->ray[tune]) (0.3.6)\n",
      "Installing collected packages: tensorboardX, tabulate\n",
      "Successfully installed tabulate-0.9.0 tensorboardX-2.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install ray[tune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5462ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ivan\\AppData\\Local\\Temp/ipykernel_28360/1382550860.py:6: DeprecationWarning: The module `ray.tune.suggest` has been moved to `ray.tune.search` and the old location will be deprecated soon. Please adjust your imports to point to the new location. Example: Do a global search and replace `ray.tune.suggest` with `ray.tune.search`.\n",
      "  from ray.tune.suggest.hyperopt import HyperOptSearch\n",
      "C:\\Users\\Ivan\\AppData\\Local\\Temp/ipykernel_28360/1382550860.py:6: DeprecationWarning: The module `ray.tune.suggest.hyperopt` has been moved to `ray.tune.search.hyperopt` and the old location will be deprecated soon. Please adjust your imports to point to the new location. Example: Do a global search and replace `ray.tune.suggest.hyperopt` with `ray.tune.search.hyperopt`.\n",
      "  from ray.tune.suggest.hyperopt import HyperOptSearch\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoConfig, DataCollatorWithPadding, TrainingArguments, Trainer, TrainerCallback\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score, mean_squared_error\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69774314",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    MODEL_NAME = 'roberta-large'\n",
    "    \n",
    "    EPOCHS = 10\n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    VAL_BATCH_SIZE = 64\n",
    "    WEIGHT_DECAY = 0.001\n",
    "    LEARNING_RATE_START = 1e-4\n",
    "    WARMUP_RATIO = 0.1\n",
    "    SCHEDULER_TYPE = 'cosine'\n",
    "    \n",
    "    SEED = 42\n",
    "    \n",
    "def set_seed_(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed_(CFG.SEED)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27a57bec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Ivan\\.cache\\huggingface\\datasets\\glue\\stsb\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39bf3c98f1644906a33055099704e5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6504193b1794c868d0bfd1c674a8a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3f5d22dd1d40fa9263f9e9ccc2982e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79da47732484421196bd380fda295b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d5eeed5e214c90a1f7e480a5aafece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x000002B54F33C5E0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af0e558ad214fec8985be3f1c18d8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd623490683b4be6bfa023e739a7b14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec5b732711d46c7a2cf33dee34fc548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('glue', 'stsb')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CFG.MODEL_NAME, \n",
    "    max_length=CFG.MAX_LEN\n",
    ")\n",
    "\n",
    "def tokenize_function(ex):\n",
    "    return tokenizer(\n",
    "        ex['sentence1'], \n",
    "        ex['sentence2'], \n",
    "        truncation=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67490aab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e866a393bef455080105a304e779137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CFG.MODEL_NAME,\n",
    "    num_labels=1,\n",
    ")\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for p in model.base_model.encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "def unfreeze_encoder(model):\n",
    "    for p in model.base_model.encoder.parameters():\n",
    "        p.requires_grad = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfbe73fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5749\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1379\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(['sentence1', 'sentence2', 'idx'])\n",
    "tokenized_datasets = tokenized_datasets.rename_columns({'label':'labels'})\n",
    "tokenized_datasets.set_format('torch')\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d6f74a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "HyperOpt must be installed! Run `pip install hyperopt`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28360/2704659596.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'maximize'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ray'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0msearch_alg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mHyperOptSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'objective'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPopulationBasedTraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'objective'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ray\\tune\\search\\hyperopt\\hyperopt_search.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, space, metric, mode, points_to_evaluate, n_initial_points, random_state_seed, gamma)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mgamma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     ):\n\u001b[1;32m--> 154\u001b[1;33m         assert (\n\u001b[0m\u001b[0;32m    155\u001b[0m             \u001b[0mhpo\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         ), \"HyperOpt must be installed! Run `pip install hyperopt`.\"\n",
      "\u001b[1;31mAssertionError\u001b[0m: HyperOpt must be installed! Run `pip install hyperopt`."
     ]
    }
   ],
   "source": [
    "## TRAINING WITH FROZEN WEIGHTS ##\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    output, labels = eval_preds\n",
    "    output = output.reshape(-1)\n",
    "    return {\n",
    "        'pearson_r': pearsonr(output, labels)[0],\n",
    "        'spearman_r': spearmanr(output, labels)[0]\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    learning_rate=CFG.LEARNING_RATE_START,\n",
    "    per_device_train_batch_size=CFG.TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=CFG.VAL_BATCH_SIZE,\n",
    "    num_train_epochs=CFG.EPOCHS,\n",
    "    output_dir=os.path.join('./frozen', CFG.MODEL_NAME),\n",
    "    weight_decay=CFG.WEIGHT_DECAY,\n",
    "    lr_scheduler_type=CFG.SCHEDULER_TYPE,\n",
    "    warmup_ratio=CFG.WARMUP_RATIO,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "freeze_encoder(model)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction='maximize',\n",
    "    backend='ray',\n",
    "    search_alg=HyperOptSearch(metric='objective', mode='max'),\n",
    "    scheduler=PopulationBasedTraining(metric='objective', mode='max')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64171374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.lr = []\n",
    "    \n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        self.lr.append(kwargs['lr_scheduler'].get_last_lr())\n",
    "        \n",
    "lr_callback = LRCallback()\n",
    "trainer.add_callback(lr_callback)\n",
    "train_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb79f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING END TO END ##\n",
    "\n",
    "#training_args_fine_tune = TrainingArguments(\n",
    "#    evaluation_strategy='epoch',\n",
    "#    save_strategy='epoch',\n",
    "#    logging_strategy='epoch',\n",
    "#    learning_rate=CFG.LEARNING_RATE_START_FINE_TUNE,\n",
    "#    per_device_train_batch_size=CFG.TRAIN_BATCH_SIZE_FINE_TUNE,\n",
    "#    per_device_eval_batch_size=CFG.VAL_BATCH_SIZE_FINE_TUNE,\n",
    "#    num_train_epochs=CFG.EPOCHS_FINE_TUNE,\n",
    "#    output_dir=os.path.join('./finetune', CFG.MODEL_NAME),\n",
    "#    weight_decay=CFG.WEIGHT_DECAY_FINE_TUNE,\n",
    "#    lr_scheduler_type=CFG.SCHEDULER_TYPE_FINE_TUNE,\n",
    "#    warmup_ratio=CFG.WARMUP_RATIO_FINE_TUNE,\n",
    "#    fp16=True\n",
    "#)\n",
    "\n",
    "#unfreeze_encoder(model)\n",
    "#trainer_fine_tune = Trainer(\n",
    "#    model,\n",
    "#    training_args,\n",
    "#    train_dataset=tokenized_datasets['train'],\n",
    "#    eval_dataset=tokenized_datasets['validation'],\n",
    "#    data_collator=data_collator,\n",
    "#    tokenizer=tokenizer,\n",
    "#    compute_metrics=compute_metrics\n",
    "#)\n",
    "\n",
    "#lr_callback_fine_tune = LRCallback()\n",
    "#trainer_fine_tune.add_callback(lr_callback_fine_tune)\n",
    "#trainer_fine_tune.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe2427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(lr_callback.lr))), lr_callback.lr, label='LR, frozen')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
