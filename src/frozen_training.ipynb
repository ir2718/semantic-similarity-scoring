{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecb557a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ray[tune] in c:\\users\\ivan\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: virtualenv in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (20.16.5)\n",
      "Requirement already satisfied: requests in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (2.26.0)\n",
      "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.43.0)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.15.3 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (3.20.1)\n",
      "Requirement already satisfied: attrs in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (21.2.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.20.3)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.0.2)\n",
      "Requirement already satisfied: frozenlist in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.3.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ivan\\appdata\\roaming\\python\\python39\\site-packages (from ray[tune]) (3.7.0)\n",
      "Requirement already satisfied: aiosignal in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.2.0)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (3.2.0)\n",
      "Requirement already satisfied: click<=8.0.4,>=7.0 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (8.0.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (1.3.4)\n",
      "Requirement already satisfied: tabulate in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (0.9.0)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from ray[tune]) (2.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ivan\\appdata\\roaming\\python\\python39\\site-packages (from click<=8.0.4,>=7.0->ray[tune]) (0.4.4)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from grpcio<=1.43.0,>=1.28.1->ray[tune]) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from jsonschema->ray[tune]) (0.18.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from jsonschema->ray[tune]) (58.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from pandas->ray[tune]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from pandas->ray[tune]) (2021.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from requests->ray[tune]) (1.26.7)\n",
      "Requirement already satisfied: platformdirs<3,>=2.4 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from virtualenv->ray[tune]) (2.5.2)\n",
      "Requirement already satisfied: distlib<1,>=0.3.5 in c:\\users\\ivan\\anaconda3\\lib\\site-packages (from virtualenv->ray[tune]) (0.3.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ivan\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install ray[tune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e5462ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoConfig, DataCollatorWithPadding, TrainingArguments, Trainer, TrainerCallback\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score, mean_squared_error\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69774314",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    MODEL_NAME = 'roberta-large'\n",
    "    \n",
    "    EPOCHS = 10\n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    VAL_BATCH_SIZE = 64\n",
    "    WEIGHT_DECAY = 0.001\n",
    "    LEARNING_RATE_START = 1e-4\n",
    "    WARMUP_RATIO = 0.1\n",
    "    SCHEDULER_TYPE = 'cosine'\n",
    "    \n",
    "    SEED = 42\n",
    "    \n",
    "def set_seed_(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed_(CFG.SEED)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27a57bec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Ivan\\.cache\\huggingface\\datasets\\glue\\stsb\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bfd27daf1a4ea1acfa20dd6e8c3895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed28316024334f098c23a9777712bd51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087a9c82e2984ed3b8360e2ae9424433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f62f4b963b54256a64cf8d2dc3e112c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('glue', 'stsb')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CFG.MODEL_NAME, \n",
    "    #max_length=CFG.MAX_LEN\n",
    ")\n",
    "\n",
    "def tokenize_function(ex):\n",
    "    return tokenizer(\n",
    "        ex['sentence1'], \n",
    "        ex['sentence2'], \n",
    "        truncation=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67490aab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CFG.MODEL_NAME,\n",
    "    num_labels=1,\n",
    ")\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for p in model.base_model.encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "def unfreeze_encoder(model):\n",
    "    for p in model.base_model.encoder.parameters():\n",
    "        p.requires_grad = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfbe73fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5749\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1379\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(['sentence1', 'sentence2', 'idx'])\n",
    "tokenized_datasets = tokenized_datasets.rename_columns({'label':'labels'})\n",
    "tokenized_datasets.set_format('torch')\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d6f74a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mixed precision training with AMP or APEX (`--fp16` or `--bf16`) and half precision evaluation (`--fp16_full_eval` or `--bf16_full_eval`) can only be used on CUDA devices.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4760/1520551527.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     }\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m training_args = TrainingArguments(\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mevaluation_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0msave_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_on_each_node, no_cuda, seed, data_seed, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, xpu_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, deepspeed, label_smoothing_factor, optim, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    935\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp16\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp16_full_eval\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbf16\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbf16_full_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m         ):\n\u001b[1;32m--> 937\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    938\u001b[0m                 \u001b[1;34m\"Mixed precision training with AMP or APEX (`--fp16` or `--bf16`) and half precision evaluation (`--fp16_full_eval` or `--bf16_full_eval`) can only be used on CUDA devices.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: Mixed precision training with AMP or APEX (`--fp16` or `--bf16`) and half precision evaluation (`--fp16_full_eval` or `--bf16_full_eval`) can only be used on CUDA devices."
     ]
    }
   ],
   "source": [
    "## TRAINING WITH FROZEN WEIGHTS ##\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    output, labels = eval_preds\n",
    "    output = output.reshape(-1)\n",
    "    return {\n",
    "        'pearson_r': pearsonr(output, labels)[0],\n",
    "        'spearman_r': spearmanr(output, labels)[0]\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    learning_rate=CFG.LEARNING_RATE_START,\n",
    "    per_device_train_batch_size=CFG.TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=CFG.VAL_BATCH_SIZE,\n",
    "    num_train_epochs=CFG.EPOCHS,\n",
    "    output_dir=os.path.join('./frozen', CFG.MODEL_NAME),\n",
    "    weight_decay=CFG.WEIGHT_DECAY,\n",
    "    lr_scheduler_type=CFG.SCHEDULER_TYPE,\n",
    "    warmup_ratio=CFG.WARMUP_RATIO,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "freeze_encoder(model)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ec113",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction='maximize',\n",
    "    backend='ray',\n",
    "    search_alg=HyperOptSearch(metric='objective', mode='max'),\n",
    "    scheduler=PopulationBasedTraining(metric='objective', mode='max')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64171374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.lr = []\n",
    "    \n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        self.lr.append(kwargs['lr_scheduler'].get_last_lr())\n",
    "\n",
    "        \n",
    "# add training with best hyperparams\n",
    "        \n",
    "lr_callback = LRCallback()\n",
    "trainer.add_callback(lr_callback)\n",
    "train_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1e8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(tokenized_datasets['test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
